{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9lw+wcYhiukZwpaGzrYVZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SauloSilva/IPCV-2020/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt4a9i1l-p_-"
      },
      "source": [
        "## **Assignment 2**\n",
        "Intructions for the assignment: [link](https://daniel-yukimura.github.io/ipcv-2020-assignment-2/)\n",
        "\n",
        "### **Part 0: Setup**\n",
        "As in the first assignment you must follow the steps for setting your Google Colab environment - [Assignment-1](https://colab.research.google.com/drive/1t8gHrqvAQ_ip2Dr6nI3JOnkgRiSVqhAd?usp=sharing).\n",
        "While preparing the notebook, we suggest you already import the modules you used in the previous clas. Also, since we'll be doing image classification with larger models, we suggest you already set the runtime of the Colab notebook to work with the GPU setting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRLLUaLs-txJ"
      },
      "source": [
        "##############################################################################\n",
        "# Setup\n",
        "##############################################################################\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7HbC2jw-z-s"
      },
      "source": [
        "### **Part 1: Convolutional Neural Networks (CNN)**\n",
        "\n",
        "In the first part of this assignment we'll be setting a CNN using PyTorch. For that we'll see how to choose the dimensions and kernel sizes, in order to correctly match the dimensions while modeling a CNN. We start by observing a base example which we'll be using throughout this part of the assignment. We'll be training the model on the MNIST dataset as in the previous assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jLCj_dU-1Gl"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNN, self).__init__()\n",
        "        self.cv1 = nn.Conv2d(1,24, kernel_size=5, stride=1, padding=2)\n",
        "        self.cv2 = nn.Conv2d(24,48, kernel_size=5, stride=1, padding=2)\n",
        "        self.fc = nn.Linear(7*7*48, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(F.max_pool2d(self.cv1(x), 2))\n",
        "        out = F.relu(F.max_pool2d(self.cv2(out), 2))\n",
        "        out = out.view(-1,7*7*48)\n",
        "        return self.fc(out)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6keI_xUI_uRi"
      },
      "source": [
        "### **Base model**:\n",
        "We'll make a lot of tests and changes based on this model, therefore we recommend that while solving this problems you create test functions in the notebook, or in a python script, in order to reuse your code.\n",
        "\n",
        "For the next exercises you should record the performance of your training using a validation step, you'll use this data for comparing the perfomance between models. We also recommend a list of hyperparameters for the initial setting\n",
        "* number of epochs: 20\n",
        "* learning rate: 0.001\n",
        "* batch size: 64\n",
        "\n",
        "**Instruction:** The goal of the following exercises is to improve the base model. Therefore, we suggest the following practices when doing your analysis:\n",
        "* After each exercise where an improvement is required, we ask that you plot a comparison graph between the perfomances of all changes in question. \n",
        "* Write a brief comment on which one is the best choice, and why is that the case. \n",
        "* You should compare them using the accuracy measure on you validation split. \n",
        "* For the cases where models have similar perfomance, the simpler (less parameters) should be the one to prevail.\n",
        "* After selecting a change, you should maintain this change for the next exercise.\n",
        "* We recommend you get use to saving and loading your models too - [Tutorial](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
        "-----\n",
        "\n",
        "1. Load the MNIST dataset and train the given base model. Here you can essentially use the same settings you have used in the first assingment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTmtPhGt_vCK"
      },
      "source": [
        "##############################################################################\n",
        "# Code\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q9JTclh_xF9"
      },
      "source": [
        "### **Optimization**\n",
        "\n",
        "2. Your first task is to improve the optimization setting you're using. You should compare the standard SGD (`torch.optim.SGD`) with the **Adam** algorithm (`torch.optim.Adam`). \n",
        "\n",
        "Adam combines other two extensions of stochastic gradient descent:\n",
        "* Adaptive Gradient Algorithm (AdaGrad)\n",
        "* Root Mean Square Propagation (RMSProp)\n",
        "\n",
        "Its parameters are:\n",
        "* **learning_rate** (or **alpha**): Also referred to as the learning rate or step size. The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training\n",
        "* **beta1**: The exponential decay rate for the first moment estimates (e.g. 0.9).\n",
        "* **beta2**: The exponential decay rate for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient (e.g. NLP and computer vision problems).\n",
        "* **epsilon**: Is a very small number to prevent any division by zero in the implementation (e.g. 10E-8). \n",
        "\n",
        "**Testing**: The recommended choice of parameters for using with Adam are the following\n",
        "* learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8\n",
        "\n",
        "But we ask you to test other values for the learning rate too, in particular the learning rates 0.0001, 0.001, 0.01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp5cc_N7_zKm"
      },
      "source": [
        "##############################################################################\n",
        "# Code\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vat-BS9_1Og"
      },
      "source": [
        "### **Number of Layers**\n",
        "Consider the following notation:\n",
        "* **24C5** means a convolution layer with 24 feature maps using a 5x5 filter and stride 1\n",
        "* **24C5S2** means a convolution layer with 24 feature maps using a 5x5 filter and stride 2\n",
        "* **P2** means max pooling using 2x2 filter and stride 2\n",
        "* **256** means fully connected dense layer with 256 units\n",
        "\n",
        "Then our base model can be described as\n",
        "* input - [24C5-P2] - [48C5-P2] - 10\n",
        "\n",
        "3. The base model contains two convolutional blocks (convolutional layer + pooling). Compare the present model with the following two:\n",
        "* input - [24C5-P2] - 256 - 10\n",
        "* input - [24C5-P2] - [48C5-P2] - [64C5-P2] - 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqD8Kofu_33S"
      },
      "source": [
        "##############################################################################\n",
        "# Code\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bTpCyHe_5zq"
      },
      "source": [
        "### **Feature maps**\n",
        "Convolutional layers work by transforming feature representations in new ones using a collection of filters (convolutional kernels). There's no rigorous rule on how to choose how many filter should one use, as is the case for the number of fully connected layers. Therefore, one must in general test different architectures, while following known reccomendations.\n",
        "\n",
        "4. Test your present model changing the number of feature maps in each convolutional layer. Here we present a recomendation of possible choices for the number of features, assuming the network has two convolutional layers\n",
        "* input - [8C5-P2] - [16C5-P2] - 10\n",
        "* input - [16C5-P2] - [32C5-P2] - 10\n",
        "* input - [24C5-P2] - [48C5-P2] - 10\n",
        "* input - [32C5-P2] - [64C5-P2] - 10\n",
        "* input - [48C5-P2] - [96C5-P2] - 10\n",
        "* input - [64C5-P2] - [128C5-P2] - 10\n",
        "\n",
        "If the model has different one, three or more layers, than the above should be adjusted equivalently for the present case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lLbkBnV_8Xl"
      },
      "source": [
        "##############################################################################\n",
        "# Code\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvWb0xIu_97O"
      },
      "source": [
        "### **Dense layer**\n",
        "As of right now, we flatten and the output of our last convolutional layer and connect it into a dense layer, that provides the classification scores.\n",
        "\n",
        "5. Add a hidden dense layer between the last convolutional layer and the last layer and compare the present model with the new one. Test hidden dense layers of sizes: 16, 64, 128, 256, 512, 1024."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTyym2aYAAnO"
      },
      "source": [
        "##############################################################################\n",
        "# Code\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds5m3NMwACwY"
      },
      "source": [
        "### **Dropout**\n",
        "Dropout is a regularization technique that consists on temporarily droping out different sets of connections, chosen randomly, during training. Heuristically, dropout can be seen as simulating the training of simpler subnetworks simultaneously. Simpler networks are less prone to overfit, and thats the purpose of using it during training.\n",
        "\n",
        "* [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
        "* [Dropout2d](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html)\n",
        "\n",
        "6. Add dropout layers in between each layer. These layers have a parameter `p` that tells the probability of a neuron to be dropped. Test the present model with the one using dropout for different values fo `p`. We recommend testing `p = 0.3, 0.4, 0.5, 0.6, 0.7`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9tJCo_VAFIq"
      },
      "source": [
        "##############################################################################\n",
        "# Code\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDseJM0LAGtq"
      },
      "source": [
        "### **Batch Normalization**\n",
        "\n",
        "Batch normalization works by computing the mean and standard deviation of each minibatch, and renormalizing the batch accordingly. It also maintain two learnable parameters that compensate the randomness coming from the batch sampling.\n",
        "* [functional.batch_norm](https://pytorch.org/docs/stable/nn.functional.html#batch-norm)\n",
        "* [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)\n",
        "\n",
        "7. Add Batch Normalization layers to the model, and compare with your current state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PzlAatTAJd2"
      },
      "source": [
        "##############################################################################\n",
        "# Code\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSb81rV5ALh9"
      },
      "source": [
        "### **Extra**:\n",
        "At this point, your network should have reached around more then 99% precision.\n",
        "Nevertheless there are some archtecture changes that can sometimes improve even more the performance of your model.\n",
        "\n",
        "8. Replace each conv. layer with 5x5 kernels in your network by two consecutive conv. layers with 3x3 kernels. For example, try replacing '32C5' with '32C3-32C3'. The two layers similarly mimic the 5x5 layer, but add more nonlinearities.\n",
        "\n",
        "9. Instead of using a maxpooling layer you can replace it by another convolutional layer that has stride 2, and therefore perfomes a subsampling, but in a learnable way. That would mean, for example, to replace'P2' with a'32C5S2'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MWfiihiANbV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}